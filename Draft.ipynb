{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozVSXkfVnttc"
   },
   "source": [
    "### Model1 CNN+LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vcLcTthG3-3"
   },
   "source": [
    "## Implementation detals\n",
    "- Input size:\n",
    "- Output size:\n",
    "<br></br>\n",
    "- Training data:\n",
    "- Validation data:\n",
    "- Test data:\n",
    "<br></br>\n",
    "- Number of epoch:\n",
    "- Batch size:\n",
    "- Learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7645,
     "status": "ok",
     "timestamp": 1671664006567,
     "user": {
      "displayName": "曹冠宇",
      "userId": "04420153168815849534"
     },
     "user_tz": -540
    },
    "id": "Ruor2R1Gn000",
    "outputId": "ef6b4f9c-e0c9-492d-cf54-51c7b279b709"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "# !pip install torchinfo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnZopCypFbgq"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zNGMVyA-KvTt"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "# input_size = (30 * 3) * 33\n",
    "# mid_size = 128 * 1\n",
    "# output_size = 200 * 120\n",
    "\n",
    "class MyEncodeCNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyEncodeCNN, self).__init__()\n",
    "\n",
    "    self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, stride=(3, 1), padding=0),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        # No Padding\n",
    "        # No Pooling\n",
    "        # In = 90 * 33 * 1\n",
    "        # Out = 30 * 31 * 32\n",
    "    )\n",
    "\n",
    "    self.layer2 = nn.Sequential(\n",
    "        nn.Conv2d(32, 64, kernel_size=5, stride=(1, 1), padding=0),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        # No Padding\n",
    "        # No Pooling\n",
    "        # In = 30 * 31 * 32\n",
    "        # Out = 26 * 27 * 64\n",
    "    )\n",
    "\n",
    "    self.layer3 = nn.Sequential(\n",
    "        nn.Conv2d(64, 32, kernel_size=5, stride=(1, 1), padding=0),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        # No Padding\n",
    "        # No Pooling\n",
    "        # In = 26 * 27 * 64\n",
    "        # Out = 22 * 23 * 32\n",
    "    )\n",
    "\n",
    "    self.layer4 = nn.Sequential(\n",
    "        nn.Conv2d(32, 1, kernel_size=5, stride=(1, 1), padding=0),\n",
    "        nn.BatchNorm2d(1),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        # No Padding\n",
    "        # No Pooling\n",
    "        # In = 22 * 23 * 32\n",
    "        # Out = 18 * 19 * 1\n",
    "    )\n",
    "\n",
    "    self.layerfc = nn.Sequential(\n",
    "        nn.Linear(18*19, 128),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(128, 32)\n",
    "    )\n",
    "\n",
    "    self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "    x = self.layer4(x)\n",
    "\n",
    "    # size_x = batch_size * 18 * 19\n",
    "    # x = self.dropout(x)\n",
    "    # x = self.layerfc(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class MyEncodeLSTM(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyEncodeLSTM, self).__init__()\n",
    "\n",
    "    self.hidden_size = 128\n",
    "    self.num_layers = 2\n",
    "\n",
    "    self.layer = nn.LSTM(36, self.hidden_size, self.num_layers, batch_first=True, dropout=0.1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out, (final_hidden_state, final_cell_state)  = self.layer(x)\n",
    "    return out[:, None, -1, :]  \n",
    "\n",
    "\n",
    "class MyEncoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyEncoder, self).__init__()\n",
    "  \n",
    "    self.cnn1 = MyEncodeCNN()\n",
    "    self.cnn2 = MyEncodeCNN()\n",
    "    self.lstm = MyEncodeLSTM()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.chunk(x, 2, dim=0)\n",
    "    x1 = self.cnn1.forward(x[0].view(-1, 1, 90, 33))\n",
    "    x2 = self.cnn2.forward(x[1].view(-1, 1, 90, 33))\n",
    "\n",
    "    # size_x = batch_size * 18 * 19\n",
    "    \n",
    "    x = torch.cat([x1, x2], dim=2).transpose(1, 2).view(-1, 19, 36)\n",
    "    # size_x = batch_size * 36  * 19\n",
    "\n",
    "    x = self.lstm.forward(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class MyDecodeLSTM(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyDecodeLSTM, self).__init__()\n",
    "\n",
    "    self.hidden_size = 64\n",
    "    self.num_layers = 2\n",
    "    self.hidden = self.init_hidden()\n",
    "\n",
    "    self.layer = nn.LSTM(128, self.hidden_size, self.num_layers, batch_first=True, dropout=0.1)\n",
    "\n",
    "  def init_hidden(self):\n",
    "    # Only for initialization\n",
    "    return (torch.zeros(1, 1, self.hidden_size), torch.zeros(1, 1, self.hidden_size))\n",
    "\n",
    "  def forward(self, x):\n",
    "    out, (final_hidden_state, final_cell_state)  = self.layer(x)\n",
    "    return out[:, -1, :]\n",
    "\n",
    "\n",
    "class MyDecodeCNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyDecodeCNN, self).__init__()\n",
    "\n",
    "    self.layerfc = nn.Sequential(\n",
    "        nn.Linear(64, 256),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(256, 1344)\n",
    "    )\n",
    "\n",
    "    self.layer1 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(1, 32, kernel_size=4, stride=2, padding=0),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        # No Unpooling\n",
    "        # Upsample size by 2x2\n",
    "        # In = 15 * 25 * 1\n",
    "        # Out = 30 * 50 * 32\n",
    "    )\n",
    "\n",
    "    self.layer2 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        # No Unpooling\n",
    "        # Upsample size by 2x2\n",
    "        # In = 30 * 50 * 32\n",
    "        # Out = 60 * 100 * 64\n",
    "    )\n",
    "\n",
    "    self.layer3 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=0),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        # No Unpooling\n",
    "        # Upsample size by 2x2\n",
    "        # In = 60 * 100 * 64\n",
    "        # Out = 120 * 200 * 32\n",
    "    )\n",
    "\n",
    "    self.layer4 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=0),\n",
    "        nn.BatchNorm2d(1),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        # No Unpooling\n",
    "        # Upsample size by 2x2\n",
    "        # In = 120 * 200 * 32\n",
    "        # Out = 240 * 400 * 1\n",
    "    )\n",
    "\n",
    "    self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layerfc(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.layer1(x.view(-1, 1, 28, 48))\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "    x = self.layer4(x)\n",
    "\n",
    "    # size_x = batch_size * 200 * 120\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class MyDecoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyDecoder, self).__init__()\n",
    "  \n",
    "    self.lstm = MyDecodeLSTM()\n",
    "    self.cnn = MyDecodeCNN()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.lstm.forward(x)\n",
    "\n",
    "    # size_x = batch_size * 375, need batch_size * 25 * 15\n",
    "\n",
    "    x = self.cnn.forward(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "  \n",
    "    self.encoder = MyEncoder()\n",
    "    self.decoder = MyDecoder()\n",
    "\n",
    "  def forward(self, x):\n",
    "    z = self.encoder(x)\n",
    "    y = self.decoder(z) \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1671636390748,
     "user": {
      "displayName": "曹冠宇",
      "userId": "04420153168815849534"
     },
     "user_tz": -540
    },
    "id": "C7tCrrQqvV3i",
    "outputId": "55705461-a3cc-4596-de22-348c04d96e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "MyModel                                       [1, 1, 120, 200]          --\n",
       "├─MyEncoder: 1-1                              [1, 1, 128]               --\n",
       "│    └─MyEncodeCNN: 2-1                       --                        48,032\n",
       "│    │    └─Sequential: 3-1                   [1, 32, 30, 31]           384\n",
       "│    │    └─Sequential: 3-2                   [1, 64, 26, 27]           51,392\n",
       "│    │    └─Sequential: 3-3                   [1, 32, 22, 23]           51,296\n",
       "│    │    └─Sequential: 3-4                   [1, 1, 18, 19]            803\n",
       "│    └─MyEncodeCNN: 2-2                       --                        48,032\n",
       "│    │    └─Sequential: 3-5                   [1, 32, 30, 31]           384\n",
       "│    │    └─Sequential: 3-6                   [1, 64, 26, 27]           51,392\n",
       "│    │    └─Sequential: 3-7                   [1, 32, 22, 23]           51,296\n",
       "│    │    └─Sequential: 3-8                   [1, 1, 18, 19]            803\n",
       "│    └─MyEncodeLSTM: 2-3                      --                        --\n",
       "│    │    └─LSTM: 3-9                         [1, 19, 128]              217,088\n",
       "├─MyDecoder: 1-2                              [1, 1, 120, 200]          --\n",
       "│    └─MyDecodeLSTM: 2-4                      --                        --\n",
       "│    │    └─LSTM: 3-10                        [1, 1, 64]                82,944\n",
       "│    └─MyDecodeCNN: 2-5                       --                        --\n",
       "│    │    └─Sequential: 3-11                  [1, 1344]                 362,048\n",
       "│    │    └─Dropout: 3-12                     [1, 1344]                 --\n",
       "│    │    └─Sequential: 3-13                  [1, 32, 58, 98]           608\n",
       "│    │    └─Sequential: 3-14                  [1, 64, 116, 196]         32,960\n",
       "│    │    └─Sequential: 3-15                  [1, 32, 118, 198]         18,528\n",
       "│    │    └─Sequential: 3-16                  [1, 1, 120, 200]          291\n",
       "===============================================================================================\n",
       "Total params: 1,018,281\n",
       "Trainable params: 1,018,281\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.32\n",
       "===============================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 41.49\n",
       "Params size (MB): 3.69\n",
       "Estimated Total Size (MB): 45.20\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = MyModel()\n",
    "summary(m1, input_size=(2, 90, 33))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzNX5ISCFhBY"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18918,
     "status": "ok",
     "timestamp": 1671664037687,
     "user": {
      "displayName": "曹冠宇",
      "userId": "04420153168815849534"
     },
     "user_tz": -540
    },
    "id": "3lkvH24vGZ8t",
    "outputId": "21098cf0-9111-4d5d-99f2-b9d453f6fe84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t.npy', 'x.npy', 'y.npy', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "datadir = '../Dataset/make00/'\n",
    "print(os.listdir(datadir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10574,
     "status": "ok",
     "timestamp": 1671664054419,
     "user": {
      "displayName": "曹冠宇",
      "userId": "04420153168815849534"
     },
     "user_tz": -540
    },
    "id": "Qv4H17YcAR3I",
    "outputId": "43bbd323-54c0-47a3-e21e-3d2029ba7bf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      "2730 780 390\n",
      "43 13 7\n"
     ]
    }
   ],
   "source": [
    "# My Dataset\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self, x_path, y_path):\n",
    "        self.data = self.load_data(x_path, y_path)\n",
    "        print('loaded')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data['x'][index], self.data['y'][index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data['x'].shape[0]\n",
    "\n",
    "    def load_data(self, x_path, y_path):\n",
    "        x = np.load(x_path)\n",
    "        y = np.load(y_path)\n",
    "\n",
    "        if x.shape[0] == y.shape[0]:\n",
    "            total_count = x.shape[0]\n",
    "        else:\n",
    "            print(x.shape, y.shape, \"lengths not equal!\")\n",
    "\n",
    "        return {'x':x, 'y':y}\n",
    "    \n",
    "\n",
    "mydata = MyDataset(datadir + 'x.npy', datadir + 'y.npy')\n",
    "\n",
    "train_size = int(len(mydata) * 0.7)\n",
    "valid_size = int(len(mydata) * 0.2)\n",
    "test_size = int(len(mydata)) - train_size - valid_size\n",
    "train_dataset, valid_dataset, test_dataset = Data.random_split(mydata, [train_size, valid_size, test_size])\n",
    "print(train_size, valid_size, test_size)\n",
    "\n",
    "train_loader = Data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = Data.DataLoader(valid_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = Data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(len(train_loader), len(valid_loader), len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6H9dFQc8LhN1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900, 2, 90, 33)\n",
      "(3900, 120, 200)\n"
     ]
    }
   ],
   "source": [
    "print(mydata.data['x'].shape)\n",
    "print(mydata.data['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy-67GwAFj5T"
   },
   "source": [
    "## Running gears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "P5GAqZF5D3lL"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "class MyArgs:\n",
    "  def __init__(self, epochs=10, learning_rate=0.001):\n",
    "    self.epochs = epochs\n",
    "    self.learning_rate = learning_rate\n",
    "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "args = MyArgs(epochs=300, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lCvRd4pOFBzc"
   },
   "outputs": [],
   "source": [
    "# Model and Loss\n",
    "\n",
    "model = MyModel().to(args.device)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_epochs_loss = []\n",
    "valid_epochs_loss = []\n",
    "\n",
    "# early_stopping = EarlyStopping(patience=args.patience,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UljtLCZzFR4P",
    "outputId": "02d3671f-7293-422f-de88-c0bd239043bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0/300,0/43of train, loss=548630.0\n",
      "epoch=0/300,21/43of train, loss=548432.3125\n",
      "epoch=0/300,42/43of train, loss=493182.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([12, 120, 200])) that is different to the input size (torch.Size([12, 1, 120, 200])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/300,0/43of train, loss=556467.9375\n",
      "epoch=1/300,21/43of train, loss=505506.21875\n",
      "epoch=1/300,42/43of train, loss=567526.0\n",
      "epoch=2/300,0/43of train, loss=511446.53125\n",
      "epoch=2/300,21/43of train, loss=565820.25\n",
      "epoch=2/300,42/43of train, loss=566095.0\n",
      "epoch=3/300,0/43of train, loss=523210.5\n",
      "epoch=3/300,21/43of train, loss=519848.28125\n",
      "epoch=3/300,42/43of train, loss=555933.875\n",
      "epoch=4/300,0/43of train, loss=676872.1875\n",
      "epoch=4/300,21/43of train, loss=576053.6875\n",
      "epoch=4/300,42/43of train, loss=480203.03125\n",
      "epoch=5/300,0/43of train, loss=489625.4375\n",
      "epoch=5/300,21/43of train, loss=513418.4375\n",
      "epoch=5/300,42/43of train, loss=519168.78125\n",
      "epoch=6/300,0/43of train, loss=516934.3125\n",
      "epoch=6/300,21/43of train, loss=556771.9375\n",
      "epoch=6/300,42/43of train, loss=575285.8125\n",
      "epoch=7/300,0/43of train, loss=575031.625\n",
      "epoch=7/300,21/43of train, loss=513949.34375\n",
      "epoch=7/300,42/43of train, loss=479787.4375\n",
      "epoch=8/300,0/43of train, loss=536300.8125\n",
      "epoch=8/300,21/43of train, loss=532048.5\n",
      "epoch=8/300,42/43of train, loss=511266.90625\n",
      "epoch=9/300,0/43of train, loss=493378.5\n",
      "epoch=9/300,21/43of train, loss=586092.9375\n",
      "epoch=9/300,42/43of train, loss=495999.625\n",
      "epoch=10/300,0/43of train, loss=493407.46875\n",
      "epoch=10/300,21/43of train, loss=572949.1875\n",
      "epoch=10/300,42/43of train, loss=579720.25\n",
      "epoch=11/300,0/43of train, loss=498023.375\n",
      "epoch=11/300,21/43of train, loss=530612.125\n",
      "epoch=11/300,42/43of train, loss=565194.5625\n",
      "epoch=12/300,0/43of train, loss=585459.25\n",
      "epoch=12/300,21/43of train, loss=519661.90625\n",
      "epoch=12/300,42/43of train, loss=495358.75\n",
      "epoch=13/300,0/43of train, loss=557338.375\n",
      "epoch=13/300,21/43of train, loss=493823.46875\n",
      "epoch=13/300,42/43of train, loss=610242.3125\n",
      "epoch=14/300,0/43of train, loss=574153.5\n",
      "epoch=14/300,21/43of train, loss=514906.625\n",
      "epoch=14/300,42/43of train, loss=477355.96875\n",
      "epoch=15/300,0/43of train, loss=509837.3125\n",
      "epoch=15/300,21/43of train, loss=543724.4375\n",
      "epoch=15/300,42/43of train, loss=538887.5\n",
      "epoch=16/300,0/43of train, loss=639771.125\n",
      "epoch=16/300,21/43of train, loss=489736.28125\n",
      "epoch=16/300,42/43of train, loss=751858.1875\n",
      "epoch=17/300,0/43of train, loss=496271.875\n",
      "epoch=17/300,21/43of train, loss=552824.375\n",
      "epoch=17/300,42/43of train, loss=536059.75\n",
      "epoch=18/300,0/43of train, loss=558701.875\n",
      "epoch=18/300,21/43of train, loss=498491.71875\n",
      "epoch=18/300,42/43of train, loss=534608.3125\n",
      "epoch=19/300,0/43of train, loss=494265.34375\n",
      "epoch=19/300,21/43of train, loss=501376.09375\n",
      "epoch=19/300,42/43of train, loss=512677.78125\n",
      "epoch=20/300,0/43of train, loss=574906.9375\n",
      "epoch=20/300,21/43of train, loss=515002.65625\n",
      "epoch=20/300,42/43of train, loss=529501.875\n",
      "epoch=21/300,0/43of train, loss=563647.1875\n",
      "epoch=21/300,21/43of train, loss=486346.09375\n",
      "epoch=21/300,42/43of train, loss=552895.0625\n",
      "epoch=22/300,0/43of train, loss=508854.1875\n",
      "epoch=22/300,21/43of train, loss=501334.78125\n",
      "epoch=22/300,42/43of train, loss=540966.4375\n",
      "epoch=23/300,0/43of train, loss=581532.4375\n",
      "epoch=23/300,21/43of train, loss=676568.9375\n",
      "epoch=23/300,42/43of train, loss=546009.9375\n",
      "epoch=24/300,0/43of train, loss=543933.5\n",
      "epoch=24/300,21/43of train, loss=504192.0\n",
      "epoch=24/300,42/43of train, loss=558472.8125\n",
      "epoch=25/300,0/43of train, loss=525290.5625\n",
      "epoch=25/300,21/43of train, loss=512324.6875\n",
      "epoch=25/300,42/43of train, loss=529498.625\n",
      "epoch=26/300,0/43of train, loss=536949.5625\n",
      "epoch=26/300,21/43of train, loss=498310.875\n",
      "epoch=26/300,42/43of train, loss=687099.9375\n",
      "epoch=27/300,0/43of train, loss=488619.25\n",
      "epoch=27/300,21/43of train, loss=518984.3125\n",
      "epoch=27/300,42/43of train, loss=489723.75\n",
      "epoch=28/300,0/43of train, loss=529998.25\n",
      "epoch=28/300,21/43of train, loss=470131.65625\n",
      "epoch=28/300,42/43of train, loss=441765.3125\n",
      "epoch=29/300,0/43of train, loss=587744.6875\n",
      "epoch=29/300,21/43of train, loss=500842.25\n",
      "epoch=29/300,42/43of train, loss=522730.59375\n",
      "epoch=30/300,0/43of train, loss=558022.375\n",
      "epoch=30/300,21/43of train, loss=623569.9375\n",
      "epoch=30/300,42/43of train, loss=529103.75\n",
      "epoch=31/300,0/43of train, loss=463100.65625\n",
      "epoch=31/300,21/43of train, loss=469267.03125\n",
      "epoch=31/300,42/43of train, loss=680371.4375\n",
      "epoch=32/300,0/43of train, loss=526425.75\n",
      "epoch=32/300,21/43of train, loss=580468.0625\n",
      "epoch=32/300,42/43of train, loss=547240.75\n",
      "epoch=33/300,0/43of train, loss=697975.6875\n",
      "epoch=33/300,21/43of train, loss=499242.84375\n",
      "epoch=33/300,42/43of train, loss=530699.125\n",
      "epoch=34/300,0/43of train, loss=584377.9375\n",
      "epoch=34/300,21/43of train, loss=484945.5\n",
      "epoch=34/300,42/43of train, loss=537303.4375\n",
      "epoch=35/300,0/43of train, loss=576806.8125\n",
      "epoch=35/300,21/43of train, loss=481046.1875\n",
      "epoch=35/300,42/43of train, loss=543868.4375\n",
      "epoch=36/300,0/43of train, loss=501897.75\n",
      "epoch=36/300,21/43of train, loss=487786.875\n",
      "epoch=36/300,42/43of train, loss=496535.625\n",
      "epoch=37/300,0/43of train, loss=543898.4375\n",
      "epoch=37/300,21/43of train, loss=490378.875\n",
      "epoch=37/300,42/43of train, loss=504852.65625\n",
      "epoch=38/300,0/43of train, loss=517056.84375\n",
      "epoch=38/300,21/43of train, loss=513012.8125\n",
      "epoch=38/300,42/43of train, loss=600230.8125\n",
      "epoch=39/300,0/43of train, loss=572492.125\n",
      "epoch=39/300,21/43of train, loss=506625.375\n",
      "epoch=39/300,42/43of train, loss=572150.8125\n",
      "epoch=40/300,0/43of train, loss=715940.875\n",
      "epoch=40/300,21/43of train, loss=575745.9375\n",
      "epoch=40/300,42/43of train, loss=555268.25\n",
      "epoch=41/300,0/43of train, loss=512308.0625\n",
      "epoch=41/300,21/43of train, loss=619935.375\n",
      "epoch=41/300,42/43of train, loss=487569.96875\n",
      "epoch=42/300,0/43of train, loss=652868.4375\n",
      "epoch=42/300,21/43of train, loss=534423.4375\n",
      "epoch=42/300,42/43of train, loss=552928.3125\n",
      "epoch=43/300,0/43of train, loss=501765.96875\n",
      "epoch=43/300,21/43of train, loss=506330.53125\n",
      "epoch=43/300,42/43of train, loss=532411.25\n",
      "epoch=44/300,0/43of train, loss=511163.46875\n",
      "epoch=44/300,21/43of train, loss=480804.6875\n",
      "epoch=44/300,42/43of train, loss=559251.3125\n",
      "epoch=45/300,0/43of train, loss=508869.5\n",
      "epoch=45/300,21/43of train, loss=592505.3125\n",
      "epoch=45/300,42/43of train, loss=753502.4375\n",
      "epoch=46/300,0/43of train, loss=512005.53125\n",
      "epoch=46/300,21/43of train, loss=500409.75\n",
      "epoch=46/300,42/43of train, loss=542881.6875\n",
      "epoch=47/300,0/43of train, loss=638419.6875\n",
      "epoch=47/300,21/43of train, loss=505114.53125\n",
      "epoch=47/300,42/43of train, loss=541048.25\n",
      "epoch=48/300,0/43of train, loss=499870.625\n",
      "epoch=48/300,21/43of train, loss=648380.125\n",
      "epoch=48/300,42/43of train, loss=496299.53125\n",
      "epoch=49/300,0/43of train, loss=547565.0625\n",
      "epoch=49/300,21/43of train, loss=559034.0\n",
      "epoch=49/300,42/43of train, loss=714705.6875\n",
      "epoch=50/300,0/43of train, loss=549112.0\n",
      "epoch=50/300,21/43of train, loss=575573.625\n",
      "epoch=50/300,42/43of train, loss=473236.9375\n",
      "epoch=51/300,0/43of train, loss=525553.125\n",
      "epoch=51/300,21/43of train, loss=499927.71875\n",
      "epoch=51/300,42/43of train, loss=539307.125\n",
      "epoch=52/300,0/43of train, loss=612441.5625\n",
      "epoch=52/300,21/43of train, loss=523087.0\n",
      "epoch=52/300,42/43of train, loss=752896.125\n",
      "epoch=53/300,0/43of train, loss=526794.0\n",
      "epoch=53/300,21/43of train, loss=490859.5\n",
      "epoch=53/300,42/43of train, loss=572942.0625\n",
      "epoch=54/300,0/43of train, loss=507016.875\n",
      "epoch=54/300,21/43of train, loss=542042.8125\n",
      "epoch=54/300,42/43of train, loss=551570.5625\n",
      "epoch=55/300,0/43of train, loss=467309.46875\n",
      "epoch=55/300,21/43of train, loss=538337.375\n",
      "epoch=55/300,42/43of train, loss=504018.40625\n",
      "epoch=56/300,0/43of train, loss=540700.6875\n",
      "epoch=56/300,21/43of train, loss=520707.875\n",
      "epoch=56/300,42/43of train, loss=546841.6875\n",
      "epoch=57/300,0/43of train, loss=507346.59375\n",
      "epoch=57/300,21/43of train, loss=488939.9375\n",
      "epoch=57/300,42/43of train, loss=464012.21875\n",
      "epoch=58/300,0/43of train, loss=526959.9375\n",
      "epoch=58/300,21/43of train, loss=472500.09375\n",
      "epoch=58/300,42/43of train, loss=467010.28125\n",
      "epoch=59/300,0/43of train, loss=564351.25\n",
      "epoch=59/300,21/43of train, loss=513853.40625\n",
      "epoch=59/300,42/43of train, loss=437290.4375\n",
      "epoch=60/300,0/43of train, loss=550373.0\n",
      "epoch=60/300,21/43of train, loss=499501.0625\n",
      "epoch=60/300,42/43of train, loss=569822.625\n",
      "epoch=61/300,0/43of train, loss=501964.875\n",
      "epoch=61/300,21/43of train, loss=518706.6875\n",
      "epoch=61/300,42/43of train, loss=437327.125\n",
      "epoch=62/300,0/43of train, loss=454940.15625\n",
      "epoch=62/300,21/43of train, loss=538373.4375\n",
      "epoch=62/300,42/43of train, loss=465051.59375\n",
      "epoch=63/300,0/43of train, loss=485821.0\n",
      "epoch=63/300,21/43of train, loss=493125.53125\n",
      "epoch=63/300,42/43of train, loss=452452.34375\n",
      "epoch=64/300,0/43of train, loss=522800.25\n",
      "epoch=64/300,21/43of train, loss=523851.0\n",
      "epoch=64/300,42/43of train, loss=453517.5625\n",
      "epoch=65/300,0/43of train, loss=539459.0625\n",
      "epoch=65/300,21/43of train, loss=504928.6875\n",
      "epoch=65/300,42/43of train, loss=736535.8125\n",
      "epoch=66/300,0/43of train, loss=542629.125\n",
      "epoch=66/300,21/43of train, loss=538315.0625\n",
      "epoch=66/300,42/43of train, loss=478955.8125\n",
      "epoch=67/300,0/43of train, loss=596269.5\n",
      "epoch=67/300,21/43of train, loss=572756.0\n",
      "epoch=67/300,42/43of train, loss=481760.34375\n",
      "epoch=68/300,0/43of train, loss=550946.0625\n",
      "epoch=68/300,21/43of train, loss=568514.875\n",
      "epoch=68/300,42/43of train, loss=558628.0625\n",
      "epoch=69/300,0/43of train, loss=523208.4375\n",
      "epoch=69/300,21/43of train, loss=496529.875\n",
      "epoch=69/300,42/43of train, loss=444931.0\n",
      "epoch=70/300,0/43of train, loss=497759.0625\n",
      "epoch=70/300,21/43of train, loss=656482.8125\n",
      "epoch=70/300,42/43of train, loss=492721.5625\n",
      "epoch=71/300,0/43of train, loss=609138.0\n",
      "epoch=71/300,21/43of train, loss=509541.0625\n",
      "epoch=71/300,42/43of train, loss=471169.46875\n",
      "epoch=72/300,0/43of train, loss=505040.46875\n",
      "epoch=72/300,21/43of train, loss=523264.25\n",
      "epoch=72/300,42/43of train, loss=487538.125\n",
      "epoch=73/300,0/43of train, loss=531320.5\n",
      "epoch=73/300,21/43of train, loss=527035.9375\n",
      "epoch=73/300,42/43of train, loss=446501.59375\n",
      "epoch=74/300,0/43of train, loss=483752.28125\n",
      "epoch=74/300,21/43of train, loss=583794.875\n",
      "epoch=74/300,42/43of train, loss=470900.6875\n",
      "epoch=75/300,0/43of train, loss=502062.28125\n",
      "epoch=75/300,21/43of train, loss=451967.0\n",
      "epoch=75/300,42/43of train, loss=549356.4375\n",
      "epoch=76/300,0/43of train, loss=592220.5625\n",
      "epoch=76/300,21/43of train, loss=539124.0625\n",
      "epoch=76/300,42/43of train, loss=522838.6875\n",
      "epoch=77/300,0/43of train, loss=690977.375\n",
      "epoch=77/300,21/43of train, loss=488677.1875\n",
      "epoch=77/300,42/43of train, loss=513553.375\n",
      "epoch=78/300,0/43of train, loss=493391.4375\n",
      "epoch=78/300,21/43of train, loss=573151.9375\n",
      "epoch=78/300,42/43of train, loss=524167.53125\n",
      "epoch=79/300,0/43of train, loss=479799.9375\n",
      "epoch=79/300,21/43of train, loss=628405.8125\n",
      "epoch=79/300,42/43of train, loss=509331.71875\n",
      "epoch=80/300,0/43of train, loss=546057.8125\n",
      "epoch=80/300,21/43of train, loss=468362.375\n",
      "epoch=80/300,42/43of train, loss=525087.6875\n",
      "epoch=81/300,0/43of train, loss=527211.625\n",
      "epoch=81/300,21/43of train, loss=497399.53125\n",
      "epoch=81/300,42/43of train, loss=505941.90625\n",
      "epoch=82/300,0/43of train, loss=501852.125\n",
      "epoch=82/300,21/43of train, loss=532969.8125\n",
      "epoch=82/300,42/43of train, loss=543785.25\n",
      "epoch=83/300,0/43of train, loss=537125.1875\n",
      "epoch=83/300,21/43of train, loss=514823.40625\n",
      "epoch=83/300,42/43of train, loss=538267.625\n",
      "epoch=84/300,0/43of train, loss=603270.125\n",
      "epoch=84/300,21/43of train, loss=530665.6875\n",
      "epoch=84/300,42/43of train, loss=573337.5\n",
      "epoch=85/300,0/43of train, loss=491233.65625\n",
      "epoch=85/300,21/43of train, loss=524168.53125\n",
      "epoch=85/300,42/43of train, loss=544656.5\n",
      "epoch=86/300,0/43of train, loss=554662.625\n",
      "epoch=86/300,21/43of train, loss=570998.25\n",
      "epoch=86/300,42/43of train, loss=550789.75\n",
      "epoch=87/300,0/43of train, loss=504760.3125\n",
      "epoch=87/300,21/43of train, loss=472315.0\n",
      "epoch=87/300,42/43of train, loss=516189.25\n",
      "epoch=88/300,0/43of train, loss=491262.96875\n",
      "epoch=88/300,21/43of train, loss=486924.53125\n",
      "epoch=88/300,42/43of train, loss=560812.3125\n",
      "epoch=89/300,0/43of train, loss=505339.5\n",
      "epoch=89/300,21/43of train, loss=503424.5\n",
      "epoch=89/300,42/43of train, loss=438648.34375\n",
      "epoch=90/300,0/43of train, loss=541906.625\n",
      "epoch=90/300,21/43of train, loss=534956.5625\n",
      "epoch=90/300,42/43of train, loss=520940.1875\n",
      "epoch=91/300,0/43of train, loss=477002.90625\n",
      "epoch=91/300,21/43of train, loss=521038.71875\n",
      "epoch=91/300,42/43of train, loss=542215.875\n",
      "epoch=92/300,0/43of train, loss=478124.28125\n",
      "epoch=92/300,21/43of train, loss=475461.875\n",
      "epoch=92/300,42/43of train, loss=526658.0\n",
      "epoch=93/300,0/43of train, loss=529252.1875\n",
      "epoch=93/300,21/43of train, loss=501144.40625\n",
      "epoch=93/300,42/43of train, loss=538523.8125\n",
      "epoch=94/300,0/43of train, loss=560018.4375\n",
      "epoch=94/300,21/43of train, loss=467101.34375\n",
      "epoch=94/300,42/43of train, loss=468655.21875\n",
      "epoch=95/300,0/43of train, loss=478131.875\n",
      "epoch=95/300,21/43of train, loss=606047.625\n",
      "epoch=95/300,42/43of train, loss=512093.5\n",
      "epoch=96/300,0/43of train, loss=599920.1875\n",
      "epoch=96/300,21/43of train, loss=533047.875\n",
      "epoch=96/300,42/43of train, loss=501535.1875\n",
      "epoch=97/300,0/43of train, loss=503010.9375\n",
      "epoch=97/300,21/43of train, loss=505333.96875\n",
      "epoch=97/300,42/43of train, loss=470229.1875\n",
      "epoch=98/300,0/43of train, loss=449996.28125\n",
      "epoch=98/300,21/43of train, loss=545161.3125\n",
      "epoch=98/300,42/43of train, loss=471469.0625\n",
      "epoch=99/300,0/43of train, loss=492425.0\n",
      "epoch=99/300,21/43of train, loss=510263.375\n",
      "epoch=99/300,42/43of train, loss=482649.90625\n",
      "epoch=100/300,0/43of train, loss=541024.875\n",
      "epoch=100/300,21/43of train, loss=499937.15625\n",
      "epoch=100/300,42/43of train, loss=505027.5\n",
      "epoch=101/300,0/43of train, loss=675043.4375\n",
      "epoch=101/300,21/43of train, loss=489786.5\n",
      "epoch=101/300,42/43of train, loss=517359.71875\n",
      "epoch=102/300,0/43of train, loss=497667.53125\n",
      "epoch=102/300,21/43of train, loss=538261.9375\n",
      "epoch=102/300,42/43of train, loss=532994.0\n",
      "epoch=103/300,0/43of train, loss=482282.84375\n",
      "epoch=103/300,21/43of train, loss=511355.3125\n",
      "epoch=103/300,42/43of train, loss=526273.1875\n",
      "epoch=104/300,0/43of train, loss=494991.09375\n",
      "epoch=104/300,21/43of train, loss=512686.46875\n",
      "epoch=104/300,42/43of train, loss=543949.375\n",
      "epoch=105/300,0/43of train, loss=495123.3125\n",
      "epoch=105/300,21/43of train, loss=482153.71875\n",
      "epoch=105/300,42/43of train, loss=472208.9375\n",
      "epoch=106/300,0/43of train, loss=535480.125\n",
      "epoch=106/300,21/43of train, loss=511694.65625\n",
      "epoch=106/300,42/43of train, loss=476253.59375\n",
      "epoch=107/300,0/43of train, loss=526185.5625\n",
      "epoch=107/300,21/43of train, loss=492226.375\n",
      "epoch=107/300,42/43of train, loss=579127.0\n",
      "epoch=108/300,0/43of train, loss=526700.625\n",
      "epoch=108/300,21/43of train, loss=516006.09375\n",
      "epoch=108/300,42/43of train, loss=479441.65625\n",
      "epoch=109/300,0/43of train, loss=593657.1875\n",
      "epoch=109/300,21/43of train, loss=543302.5\n",
      "epoch=109/300,42/43of train, loss=517208.28125\n",
      "epoch=110/300,0/43of train, loss=525252.625\n",
      "epoch=110/300,21/43of train, loss=508332.375\n",
      "epoch=110/300,42/43of train, loss=467736.4375\n",
      "epoch=111/300,0/43of train, loss=526561.375\n",
      "epoch=111/300,21/43of train, loss=486404.0\n",
      "epoch=111/300,42/43of train, loss=502234.125\n",
      "epoch=112/300,0/43of train, loss=481032.1875\n",
      "epoch=112/300,21/43of train, loss=464569.9375\n",
      "epoch=112/300,42/43of train, loss=604918.6875\n",
      "epoch=113/300,0/43of train, loss=502497.4375\n",
      "epoch=113/300,21/43of train, loss=576604.9375\n",
      "epoch=113/300,42/43of train, loss=450834.375\n",
      "epoch=114/300,0/43of train, loss=526688.1875\n",
      "epoch=114/300,21/43of train, loss=454453.875\n",
      "epoch=114/300,42/43of train, loss=530085.0\n",
      "epoch=115/300,0/43of train, loss=503473.65625\n",
      "epoch=115/300,21/43of train, loss=509827.75\n",
      "epoch=115/300,42/43of train, loss=524211.09375\n",
      "epoch=116/300,0/43of train, loss=475638.4375\n",
      "epoch=116/300,21/43of train, loss=470872.3125\n",
      "epoch=116/300,42/43of train, loss=551103.6875\n",
      "epoch=117/300,0/43of train, loss=501999.0\n",
      "epoch=117/300,21/43of train, loss=474036.125\n",
      "epoch=117/300,42/43of train, loss=538074.125\n",
      "epoch=118/300,0/43of train, loss=500847.875\n",
      "epoch=118/300,21/43of train, loss=564069.4375\n",
      "epoch=118/300,42/43of train, loss=485823.53125\n",
      "epoch=119/300,0/43of train, loss=467335.9375\n",
      "epoch=119/300,21/43of train, loss=504460.9375\n",
      "epoch=119/300,42/43of train, loss=554904.3125\n",
      "epoch=120/300,0/43of train, loss=690573.875\n",
      "epoch=120/300,21/43of train, loss=540818.375\n",
      "epoch=120/300,42/43of train, loss=496947.375\n",
      "epoch=121/300,0/43of train, loss=556268.6875\n",
      "epoch=121/300,21/43of train, loss=506935.625\n",
      "epoch=121/300,42/43of train, loss=477962.375\n",
      "epoch=122/300,0/43of train, loss=465035.8125\n",
      "epoch=122/300,21/43of train, loss=426215.75\n",
      "epoch=122/300,42/43of train, loss=492888.78125\n",
      "epoch=123/300,0/43of train, loss=465402.375\n",
      "epoch=123/300,21/43of train, loss=482203.65625\n",
      "epoch=123/300,42/43of train, loss=506482.5625\n",
      "epoch=124/300,0/43of train, loss=589050.6875\n",
      "epoch=124/300,21/43of train, loss=559481.875\n",
      "epoch=124/300,42/43of train, loss=474316.59375\n",
      "epoch=125/300,0/43of train, loss=511378.46875\n",
      "epoch=125/300,21/43of train, loss=515888.125\n",
      "epoch=125/300,42/43of train, loss=462243.25\n",
      "epoch=126/300,0/43of train, loss=517130.75\n",
      "epoch=126/300,21/43of train, loss=503097.8125\n",
      "epoch=126/300,42/43of train, loss=447737.59375\n",
      "epoch=127/300,0/43of train, loss=518781.59375\n",
      "epoch=127/300,21/43of train, loss=536057.0625\n",
      "epoch=127/300,42/43of train, loss=447900.0625\n",
      "epoch=128/300,0/43of train, loss=532408.3125\n",
      "epoch=128/300,21/43of train, loss=460987.125\n",
      "epoch=128/300,42/43of train, loss=506277.9375\n",
      "epoch=129/300,0/43of train, loss=539859.875\n",
      "epoch=129/300,21/43of train, loss=497115.90625\n",
      "epoch=129/300,42/43of train, loss=459819.8125\n",
      "epoch=130/300,0/43of train, loss=666286.125\n",
      "epoch=130/300,21/43of train, loss=562781.625\n",
      "epoch=130/300,42/43of train, loss=487308.65625\n",
      "epoch=131/300,0/43of train, loss=524747.125\n",
      "epoch=131/300,21/43of train, loss=499313.28125\n",
      "epoch=131/300,42/43of train, loss=545600.5625\n",
      "epoch=132/300,0/43of train, loss=520811.0\n",
      "epoch=132/300,21/43of train, loss=514061.0625\n",
      "epoch=132/300,42/43of train, loss=410699.03125\n",
      "epoch=133/300,0/43of train, loss=517742.0625\n",
      "epoch=133/300,21/43of train, loss=480920.21875\n",
      "epoch=133/300,42/43of train, loss=474217.21875\n",
      "epoch=134/300,0/43of train, loss=596788.625\n",
      "epoch=134/300,21/43of train, loss=520543.0625\n",
      "epoch=134/300,42/43of train, loss=523368.875\n",
      "epoch=135/300,0/43of train, loss=491916.28125\n",
      "epoch=135/300,21/43of train, loss=496553.9375\n",
      "epoch=135/300,42/43of train, loss=511405.0\n",
      "epoch=136/300,0/43of train, loss=481067.4375\n",
      "epoch=136/300,21/43of train, loss=408029.0\n",
      "epoch=136/300,42/43of train, loss=554080.0\n",
      "epoch=137/300,0/43of train, loss=498217.8125\n",
      "epoch=137/300,21/43of train, loss=452183.5\n",
      "epoch=137/300,42/43of train, loss=443040.90625\n",
      "epoch=138/300,0/43of train, loss=516965.375\n",
      "epoch=138/300,21/43of train, loss=465281.09375\n",
      "epoch=138/300,42/43of train, loss=505277.625\n",
      "epoch=139/300,0/43of train, loss=566748.25\n",
      "epoch=139/300,21/43of train, loss=602337.1875\n",
      "epoch=139/300,42/43of train, loss=492976.0625\n",
      "epoch=140/300,0/43of train, loss=528047.6875\n",
      "epoch=140/300,21/43of train, loss=558645.3125\n",
      "epoch=140/300,42/43of train, loss=706229.0625\n",
      "epoch=141/300,0/43of train, loss=517336.21875\n",
      "epoch=141/300,21/43of train, loss=443002.1875\n",
      "epoch=141/300,42/43of train, loss=513081.8125\n",
      "epoch=142/300,0/43of train, loss=465252.59375\n",
      "epoch=142/300,21/43of train, loss=461960.1875\n",
      "epoch=142/300,42/43of train, loss=464580.09375\n",
      "epoch=143/300,0/43of train, loss=535558.125\n",
      "epoch=143/300,21/43of train, loss=479708.1875\n",
      "epoch=143/300,42/43of train, loss=515122.6875\n",
      "epoch=144/300,0/43of train, loss=529014.9375\n",
      "epoch=144/300,21/43of train, loss=530575.6875\n",
      "epoch=144/300,42/43of train, loss=488283.03125\n",
      "epoch=145/300,0/43of train, loss=647600.125\n",
      "epoch=145/300,21/43of train, loss=518687.15625\n",
      "epoch=145/300,42/43of train, loss=463247.15625\n",
      "epoch=146/300,0/43of train, loss=506570.3125\n",
      "epoch=146/300,21/43of train, loss=532734.125\n",
      "epoch=146/300,42/43of train, loss=537638.75\n",
      "epoch=147/300,0/43of train, loss=534276.75\n",
      "epoch=147/300,21/43of train, loss=499618.125\n",
      "epoch=147/300,42/43of train, loss=510166.8125\n",
      "epoch=148/300,0/43of train, loss=529018.1875\n",
      "epoch=148/300,21/43of train, loss=488209.4375\n",
      "epoch=148/300,42/43of train, loss=529150.375\n",
      "epoch=149/300,0/43of train, loss=489831.40625\n",
      "epoch=149/300,21/43of train, loss=462416.03125\n",
      "epoch=149/300,42/43of train, loss=461828.1875\n",
      "epoch=150/300,0/43of train, loss=504122.625\n",
      "epoch=150/300,21/43of train, loss=468834.96875\n",
      "epoch=150/300,42/43of train, loss=510127.6875\n",
      "epoch=151/300,0/43of train, loss=503509.65625\n",
      "epoch=151/300,21/43of train, loss=468917.34375\n",
      "epoch=151/300,42/43of train, loss=510661.96875\n",
      "epoch=152/300,0/43of train, loss=529247.3125\n",
      "epoch=152/300,21/43of train, loss=461353.28125\n",
      "epoch=152/300,42/43of train, loss=567916.9375\n",
      "epoch=153/300,0/43of train, loss=471944.1875\n",
      "epoch=153/300,21/43of train, loss=540393.875\n",
      "epoch=153/300,42/43of train, loss=471024.03125\n",
      "epoch=154/300,0/43of train, loss=675378.1875\n",
      "epoch=154/300,21/43of train, loss=557022.375\n",
      "epoch=154/300,42/43of train, loss=470068.0\n",
      "epoch=155/300,0/43of train, loss=527036.6875\n",
      "epoch=155/300,21/43of train, loss=523449.15625\n",
      "epoch=155/300,42/43of train, loss=477136.5625\n",
      "epoch=156/300,0/43of train, loss=530777.1875\n",
      "epoch=156/300,21/43of train, loss=474802.34375\n",
      "epoch=156/300,42/43of train, loss=554879.125\n",
      "epoch=157/300,0/43of train, loss=505023.1875\n",
      "epoch=157/300,21/43of train, loss=504936.53125\n",
      "epoch=157/300,42/43of train, loss=466804.625\n",
      "epoch=158/300,0/43of train, loss=462573.9375\n",
      "epoch=158/300,21/43of train, loss=497278.6875\n",
      "epoch=158/300,42/43of train, loss=486655.0\n",
      "epoch=159/300,0/43of train, loss=507559.9375\n",
      "epoch=159/300,21/43of train, loss=514016.6875\n",
      "epoch=159/300,42/43of train, loss=526613.9375\n",
      "epoch=160/300,0/43of train, loss=508295.5\n",
      "epoch=160/300,21/43of train, loss=465966.875\n",
      "epoch=160/300,42/43of train, loss=564600.375\n",
      "epoch=161/300,0/43of train, loss=503573.1875\n",
      "epoch=161/300,21/43of train, loss=525662.1875\n",
      "epoch=161/300,42/43of train, loss=404985.03125\n",
      "epoch=162/300,0/43of train, loss=484508.75\n",
      "epoch=162/300,21/43of train, loss=501610.15625\n",
      "epoch=162/300,42/43of train, loss=525184.1875\n",
      "epoch=163/300,0/43of train, loss=525174.375\n",
      "epoch=163/300,21/43of train, loss=502197.25\n",
      "epoch=163/300,42/43of train, loss=530940.375\n",
      "epoch=164/300,0/43of train, loss=481310.15625\n",
      "epoch=164/300,21/43of train, loss=588167.5\n",
      "epoch=164/300,42/43of train, loss=491236.3125\n",
      "epoch=165/300,0/43of train, loss=491011.84375\n",
      "epoch=165/300,21/43of train, loss=448475.6875\n",
      "epoch=165/300,42/43of train, loss=460653.78125\n",
      "epoch=166/300,0/43of train, loss=521020.84375\n",
      "epoch=166/300,21/43of train, loss=468625.09375\n",
      "epoch=166/300,42/43of train, loss=439446.09375\n",
      "epoch=167/300,0/43of train, loss=502321.875\n",
      "epoch=167/300,21/43of train, loss=490729.6875\n",
      "epoch=167/300,42/43of train, loss=467576.25\n",
      "epoch=168/300,0/43of train, loss=482147.25\n",
      "epoch=168/300,21/43of train, loss=504577.65625\n",
      "epoch=168/300,42/43of train, loss=545874.1875\n",
      "epoch=169/300,0/43of train, loss=463992.65625\n",
      "epoch=169/300,21/43of train, loss=469591.875\n",
      "epoch=169/300,42/43of train, loss=440310.46875\n",
      "epoch=170/300,0/43of train, loss=492924.625\n",
      "epoch=170/300,21/43of train, loss=494669.5625\n",
      "epoch=170/300,42/43of train, loss=789637.0\n",
      "epoch=171/300,0/43of train, loss=526641.9375\n",
      "epoch=171/300,21/43of train, loss=472203.5\n",
      "epoch=171/300,42/43of train, loss=452128.90625\n",
      "epoch=172/300,0/43of train, loss=513290.4375\n",
      "epoch=172/300,21/43of train, loss=476096.40625\n",
      "epoch=172/300,42/43of train, loss=499537.53125\n",
      "epoch=173/300,0/43of train, loss=420875.375\n",
      "epoch=173/300,21/43of train, loss=459975.375\n",
      "epoch=173/300,42/43of train, loss=501476.71875\n",
      "epoch=174/300,0/43of train, loss=459165.1875\n",
      "epoch=174/300,21/43of train, loss=473005.46875\n",
      "epoch=174/300,42/43of train, loss=526652.625\n",
      "epoch=175/300,0/43of train, loss=507773.9375\n",
      "epoch=175/300,21/43of train, loss=486170.40625\n",
      "epoch=175/300,42/43of train, loss=666659.4375\n",
      "epoch=176/300,0/43of train, loss=499203.0625\n",
      "epoch=176/300,21/43of train, loss=515660.0625\n",
      "epoch=176/300,42/43of train, loss=505099.1875\n",
      "epoch=177/300,0/43of train, loss=498391.9375\n",
      "epoch=177/300,21/43of train, loss=515844.34375\n",
      "epoch=177/300,42/43of train, loss=498598.09375\n",
      "epoch=178/300,0/43of train, loss=483867.125\n",
      "epoch=178/300,21/43of train, loss=506791.875\n",
      "epoch=178/300,42/43of train, loss=485425.34375\n",
      "epoch=179/300,0/43of train, loss=517589.75\n",
      "epoch=179/300,21/43of train, loss=487011.0625\n",
      "epoch=179/300,42/43of train, loss=499915.75\n",
      "epoch=180/300,0/43of train, loss=492620.46875\n",
      "epoch=180/300,21/43of train, loss=482900.59375\n",
      "epoch=180/300,42/43of train, loss=538562.25\n",
      "epoch=181/300,0/43of train, loss=543674.25\n",
      "epoch=181/300,21/43of train, loss=476898.25\n",
      "epoch=181/300,42/43of train, loss=503733.59375\n",
      "epoch=182/300,0/43of train, loss=464186.1875\n",
      "epoch=182/300,21/43of train, loss=488756.71875\n",
      "epoch=182/300,42/43of train, loss=573265.125\n",
      "epoch=183/300,0/43of train, loss=514124.125\n",
      "epoch=183/300,21/43of train, loss=481265.15625\n",
      "epoch=183/300,42/43of train, loss=488768.9375\n",
      "epoch=184/300,0/43of train, loss=508730.28125\n",
      "epoch=184/300,21/43of train, loss=541815.1875\n",
      "epoch=184/300,42/43of train, loss=675627.1875\n",
      "epoch=185/300,0/43of train, loss=529035.75\n",
      "epoch=185/300,21/43of train, loss=502611.96875\n",
      "epoch=185/300,42/43of train, loss=498064.59375\n",
      "epoch=186/300,0/43of train, loss=501248.84375\n",
      "epoch=186/300,21/43of train, loss=516963.3125\n",
      "epoch=186/300,42/43of train, loss=488368.03125\n",
      "epoch=187/300,0/43of train, loss=550516.5625\n",
      "epoch=187/300,21/43of train, loss=551034.625\n",
      "epoch=187/300,42/43of train, loss=438258.40625\n",
      "epoch=188/300,0/43of train, loss=462738.84375\n",
      "epoch=188/300,21/43of train, loss=499550.875\n",
      "epoch=188/300,42/43of train, loss=465731.21875\n",
      "epoch=189/300,0/43of train, loss=481694.96875\n",
      "epoch=189/300,21/43of train, loss=481544.65625\n",
      "epoch=189/300,42/43of train, loss=512029.9375\n",
      "epoch=190/300,0/43of train, loss=486152.875\n",
      "epoch=190/300,21/43of train, loss=513511.15625\n",
      "epoch=190/300,42/43of train, loss=491574.6875\n",
      "epoch=191/300,0/43of train, loss=488435.15625\n",
      "epoch=191/300,21/43of train, loss=488365.21875\n",
      "epoch=191/300,42/43of train, loss=493228.6875\n",
      "epoch=192/300,0/43of train, loss=519610.84375\n",
      "epoch=192/300,21/43of train, loss=524821.25\n",
      "epoch=192/300,42/43of train, loss=478629.34375\n",
      "epoch=193/300,0/43of train, loss=474655.40625\n",
      "epoch=193/300,21/43of train, loss=472320.375\n",
      "epoch=193/300,42/43of train, loss=499292.84375\n",
      "epoch=194/300,0/43of train, loss=526831.9375\n",
      "epoch=194/300,21/43of train, loss=468729.9375\n",
      "epoch=194/300,42/43of train, loss=495675.1875\n",
      "epoch=195/300,0/43of train, loss=513850.09375\n",
      "epoch=195/300,21/43of train, loss=479678.625\n",
      "epoch=195/300,42/43of train, loss=722915.3125\n",
      "epoch=196/300,0/43of train, loss=526179.4375\n",
      "epoch=196/300,21/43of train, loss=468483.25\n",
      "epoch=196/300,42/43of train, loss=490601.03125\n",
      "epoch=197/300,0/43of train, loss=463392.8125\n",
      "epoch=197/300,21/43of train, loss=484106.96875\n",
      "epoch=197/300,42/43of train, loss=490437.75\n",
      "epoch=198/300,0/43of train, loss=530653.625\n",
      "epoch=198/300,21/43of train, loss=463245.40625\n",
      "epoch=198/300,42/43of train, loss=519699.09375\n",
      "epoch=199/300,0/43of train, loss=481271.28125\n",
      "epoch=199/300,21/43of train, loss=463373.59375\n",
      "epoch=199/300,42/43of train, loss=554740.75\n",
      "epoch=200/300,0/43of train, loss=558188.6875\n",
      "epoch=200/300,21/43of train, loss=575884.625\n",
      "epoch=200/300,42/43of train, loss=535326.1875\n",
      "epoch=201/300,0/43of train, loss=535418.4375\n",
      "epoch=201/300,21/43of train, loss=487567.75\n",
      "epoch=201/300,42/43of train, loss=487276.5625\n",
      "epoch=202/300,0/43of train, loss=486739.84375\n",
      "epoch=202/300,21/43of train, loss=494912.21875\n",
      "epoch=202/300,42/43of train, loss=517605.3125\n",
      "epoch=203/300,0/43of train, loss=503546.375\n",
      "epoch=203/300,21/43of train, loss=585123.75\n",
      "epoch=203/300,42/43of train, loss=512057.5\n",
      "epoch=204/300,0/43of train, loss=542190.375\n",
      "epoch=204/300,21/43of train, loss=434190.9375\n",
      "epoch=204/300,42/43of train, loss=524730.1875\n",
      "epoch=205/300,0/43of train, loss=614734.125\n",
      "epoch=205/300,21/43of train, loss=517486.84375\n",
      "epoch=205/300,42/43of train, loss=476902.59375\n",
      "epoch=206/300,0/43of train, loss=487098.375\n",
      "epoch=206/300,21/43of train, loss=480976.75\n",
      "epoch=206/300,42/43of train, loss=429379.3125\n",
      "epoch=207/300,0/43of train, loss=479147.65625\n",
      "epoch=207/300,21/43of train, loss=508754.625\n",
      "epoch=207/300,42/43of train, loss=510515.0\n",
      "epoch=208/300,0/43of train, loss=473023.46875\n",
      "epoch=208/300,21/43of train, loss=511806.625\n",
      "epoch=208/300,42/43of train, loss=469712.53125\n",
      "epoch=209/300,0/43of train, loss=554187.9375\n",
      "epoch=209/300,21/43of train, loss=481329.5\n",
      "epoch=209/300,42/43of train, loss=525257.3125\n",
      "epoch=210/300,0/43of train, loss=570491.5625\n",
      "epoch=210/300,21/43of train, loss=448764.375\n",
      "epoch=210/300,42/43of train, loss=474058.90625\n",
      "epoch=211/300,0/43of train, loss=588067.75\n",
      "epoch=211/300,21/43of train, loss=518262.34375\n",
      "epoch=211/300,42/43of train, loss=508961.34375\n",
      "epoch=212/300,0/43of train, loss=492480.40625\n",
      "epoch=212/300,21/43of train, loss=500921.6875\n",
      "epoch=212/300,42/43of train, loss=556443.4375\n",
      "epoch=213/300,0/43of train, loss=446503.34375\n",
      "epoch=213/300,21/43of train, loss=446712.6875\n",
      "epoch=213/300,42/43of train, loss=498920.875\n",
      "epoch=214/300,0/43of train, loss=468761.4375\n",
      "epoch=214/300,21/43of train, loss=468766.96875\n",
      "epoch=214/300,42/43of train, loss=490405.59375\n",
      "epoch=215/300,0/43of train, loss=529402.9375\n",
      "epoch=215/300,21/43of train, loss=513033.71875\n",
      "epoch=215/300,42/43of train, loss=459772.9375\n",
      "epoch=216/300,0/43of train, loss=482487.03125\n",
      "epoch=216/300,21/43of train, loss=464896.09375\n",
      "epoch=216/300,42/43of train, loss=458467.28125\n",
      "epoch=217/300,0/43of train, loss=494375.34375\n",
      "epoch=217/300,21/43of train, loss=488783.40625\n",
      "epoch=217/300,42/43of train, loss=495697.96875\n",
      "epoch=218/300,0/43of train, loss=424968.34375\n",
      "epoch=218/300,21/43of train, loss=532779.25\n",
      "epoch=218/300,42/43of train, loss=458655.5\n",
      "epoch=219/300,0/43of train, loss=451223.25\n",
      "epoch=219/300,21/43of train, loss=500429.8125\n",
      "epoch=219/300,42/43of train, loss=513361.9375\n",
      "epoch=220/300,0/43of train, loss=502698.0625\n",
      "epoch=220/300,21/43of train, loss=441739.9375\n",
      "epoch=220/300,42/43of train, loss=504102.6875\n",
      "epoch=221/300,0/43of train, loss=470855.0\n",
      "epoch=221/300,21/43of train, loss=459261.6875\n",
      "epoch=221/300,42/43of train, loss=470772.78125\n",
      "epoch=222/300,0/43of train, loss=643152.5625\n",
      "epoch=222/300,21/43of train, loss=480604.15625\n",
      "epoch=222/300,42/43of train, loss=512444.0625\n",
      "epoch=223/300,0/43of train, loss=578831.9375\n",
      "epoch=223/300,21/43of train, loss=440266.5\n",
      "epoch=223/300,42/43of train, loss=501786.4375\n",
      "epoch=224/300,0/43of train, loss=496060.25\n",
      "epoch=224/300,21/43of train, loss=476036.6875\n",
      "epoch=224/300,42/43of train, loss=639253.625\n",
      "epoch=225/300,0/43of train, loss=525400.3125\n",
      "epoch=225/300,21/43of train, loss=504665.03125\n",
      "epoch=225/300,42/43of train, loss=453695.34375\n",
      "epoch=226/300,0/43of train, loss=539444.3125\n",
      "epoch=226/300,21/43of train, loss=486641.5625\n",
      "epoch=226/300,42/43of train, loss=535502.125\n",
      "epoch=227/300,0/43of train, loss=490141.25\n",
      "epoch=227/300,21/43of train, loss=548138.4375\n",
      "epoch=227/300,42/43of train, loss=488412.125\n",
      "epoch=228/300,0/43of train, loss=487130.5\n",
      "epoch=228/300,21/43of train, loss=487445.15625\n",
      "epoch=228/300,42/43of train, loss=461941.03125\n",
      "epoch=229/300,0/43of train, loss=488993.28125\n",
      "epoch=229/300,21/43of train, loss=506177.09375\n",
      "epoch=229/300,42/43of train, loss=491857.53125\n",
      "epoch=230/300,0/43of train, loss=491628.53125\n",
      "epoch=230/300,21/43of train, loss=471442.84375\n",
      "epoch=230/300,42/43of train, loss=519327.09375\n",
      "epoch=231/300,0/43of train, loss=569059.125\n",
      "epoch=231/300,21/43of train, loss=499176.78125\n",
      "epoch=231/300,42/43of train, loss=477075.25\n",
      "epoch=232/300,0/43of train, loss=489974.53125\n",
      "epoch=232/300,21/43of train, loss=477010.375\n",
      "epoch=232/300,42/43of train, loss=497067.4375\n",
      "epoch=233/300,0/43of train, loss=491656.21875\n",
      "epoch=233/300,21/43of train, loss=506131.78125\n",
      "epoch=233/300,42/43of train, loss=434645.96875\n",
      "epoch=234/300,0/43of train, loss=519785.6875\n",
      "epoch=234/300,21/43of train, loss=477409.96875\n",
      "epoch=234/300,42/43of train, loss=527457.625\n",
      "epoch=235/300,0/43of train, loss=475654.46875\n",
      "epoch=235/300,21/43of train, loss=484300.96875\n",
      "epoch=235/300,42/43of train, loss=478839.71875\n",
      "epoch=236/300,0/43of train, loss=480794.28125\n",
      "epoch=236/300,21/43of train, loss=593403.375\n",
      "epoch=236/300,42/43of train, loss=543413.25\n",
      "epoch=237/300,0/43of train, loss=470229.1875\n",
      "epoch=237/300,21/43of train, loss=602356.1875\n",
      "epoch=237/300,42/43of train, loss=460467.4375\n",
      "epoch=238/300,0/43of train, loss=504704.59375\n",
      "epoch=238/300,21/43of train, loss=502399.75\n",
      "epoch=238/300,42/43of train, loss=458914.125\n",
      "epoch=239/300,0/43of train, loss=534411.875\n",
      "epoch=239/300,21/43of train, loss=530353.625\n",
      "epoch=239/300,42/43of train, loss=510441.03125\n",
      "epoch=240/300,0/43of train, loss=511362.09375\n",
      "epoch=240/300,21/43of train, loss=471256.8125\n",
      "epoch=240/300,42/43of train, loss=513724.78125\n",
      "epoch=241/300,0/43of train, loss=597733.625\n",
      "epoch=241/300,21/43of train, loss=530711.6875\n",
      "epoch=241/300,42/43of train, loss=483004.875\n",
      "epoch=242/300,0/43of train, loss=491825.375\n",
      "epoch=242/300,21/43of train, loss=497815.875\n",
      "epoch=242/300,42/43of train, loss=501395.78125\n",
      "epoch=243/300,0/43of train, loss=464090.1875\n",
      "epoch=243/300,21/43of train, loss=449560.1875\n",
      "epoch=243/300,42/43of train, loss=458875.34375\n",
      "epoch=244/300,0/43of train, loss=540223.8125\n",
      "epoch=244/300,21/43of train, loss=544143.5\n",
      "epoch=244/300,42/43of train, loss=549908.875\n",
      "epoch=245/300,0/43of train, loss=529852.3125\n",
      "epoch=245/300,21/43of train, loss=510180.0\n",
      "epoch=245/300,42/43of train, loss=486057.25\n",
      "epoch=246/300,0/43of train, loss=475589.125\n",
      "epoch=246/300,21/43of train, loss=492175.875\n",
      "epoch=246/300,42/43of train, loss=448019.375\n",
      "epoch=247/300,0/43of train, loss=574537.8125\n",
      "epoch=247/300,21/43of train, loss=479827.375\n",
      "epoch=247/300,42/43of train, loss=781783.75\n",
      "epoch=248/300,0/43of train, loss=511711.0625\n",
      "epoch=248/300,21/43of train, loss=444848.25\n",
      "epoch=248/300,42/43of train, loss=553358.9375\n",
      "epoch=249/300,0/43of train, loss=451040.125\n",
      "epoch=249/300,21/43of train, loss=486048.34375\n",
      "epoch=249/300,42/43of train, loss=406641.90625\n",
      "epoch=250/300,0/43of train, loss=480744.4375\n",
      "epoch=250/300,21/43of train, loss=499800.78125\n",
      "epoch=250/300,42/43of train, loss=485118.46875\n",
      "epoch=251/300,0/43of train, loss=460143.0625\n",
      "epoch=251/300,21/43of train, loss=494510.28125\n",
      "epoch=251/300,42/43of train, loss=463243.03125\n",
      "epoch=252/300,0/43of train, loss=462932.71875\n",
      "epoch=252/300,21/43of train, loss=458219.84375\n",
      "epoch=252/300,42/43of train, loss=524762.6875\n",
      "epoch=253/300,0/43of train, loss=463238.875\n",
      "epoch=253/300,21/43of train, loss=492150.53125\n",
      "epoch=253/300,42/43of train, loss=528222.125\n",
      "epoch=254/300,0/43of train, loss=486026.90625\n",
      "epoch=254/300,21/43of train, loss=513313.375\n",
      "epoch=254/300,42/43of train, loss=515584.8125\n",
      "epoch=255/300,0/43of train, loss=492538.78125\n",
      "epoch=255/300,21/43of train, loss=479038.28125\n",
      "epoch=255/300,42/43of train, loss=476407.15625\n",
      "epoch=256/300,0/43of train, loss=615095.875\n",
      "epoch=256/300,21/43of train, loss=498073.5\n",
      "epoch=256/300,42/43of train, loss=481565.28125\n",
      "epoch=257/300,0/43of train, loss=455198.53125\n",
      "epoch=257/300,21/43of train, loss=424047.1875\n",
      "epoch=257/300,42/43of train, loss=468106.28125\n",
      "epoch=258/300,0/43of train, loss=601971.5625\n",
      "epoch=258/300,21/43of train, loss=487914.5\n",
      "epoch=258/300,42/43of train, loss=525893.5\n",
      "epoch=259/300,0/43of train, loss=487108.78125\n",
      "epoch=259/300,21/43of train, loss=543909.0\n",
      "epoch=259/300,42/43of train, loss=418951.875\n",
      "epoch=260/300,0/43of train, loss=514604.9375\n",
      "epoch=260/300,21/43of train, loss=496522.03125\n",
      "epoch=260/300,42/43of train, loss=414200.8125\n",
      "epoch=261/300,0/43of train, loss=513299.875\n",
      "epoch=261/300,21/43of train, loss=517284.6875\n",
      "epoch=261/300,42/43of train, loss=473675.59375\n",
      "epoch=262/300,0/43of train, loss=488194.46875\n",
      "epoch=262/300,21/43of train, loss=480451.40625\n",
      "epoch=262/300,42/43of train, loss=443768.09375\n",
      "epoch=263/300,0/43of train, loss=520254.5\n",
      "epoch=263/300,21/43of train, loss=631654.75\n",
      "epoch=263/300,42/43of train, loss=494694.8125\n",
      "epoch=264/300,0/43of train, loss=485284.09375\n",
      "epoch=264/300,21/43of train, loss=562377.875\n",
      "epoch=264/300,42/43of train, loss=510006.15625\n",
      "epoch=265/300,0/43of train, loss=512464.34375\n",
      "epoch=265/300,21/43of train, loss=509798.65625\n",
      "epoch=265/300,42/43of train, loss=486570.59375\n",
      "epoch=266/300,0/43of train, loss=443146.75\n",
      "epoch=266/300,21/43of train, loss=516614.875\n",
      "epoch=266/300,42/43of train, loss=543432.0\n",
      "epoch=267/300,0/43of train, loss=517991.0625\n",
      "epoch=267/300,21/43of train, loss=477831.03125\n",
      "epoch=267/300,42/43of train, loss=517062.25\n",
      "epoch=268/300,0/43of train, loss=554474.875\n",
      "epoch=268/300,21/43of train, loss=500984.5625\n",
      "epoch=268/300,42/43of train, loss=467656.1875\n",
      "epoch=269/300,0/43of train, loss=501730.03125\n",
      "epoch=269/300,21/43of train, loss=492433.5625\n",
      "epoch=269/300,42/43of train, loss=736288.75\n",
      "epoch=270/300,0/43of train, loss=518788.90625\n",
      "epoch=270/300,21/43of train, loss=448162.8125\n",
      "epoch=270/300,42/43of train, loss=488428.3125\n",
      "epoch=271/300,0/43of train, loss=453938.09375\n",
      "epoch=271/300,21/43of train, loss=480777.25\n",
      "epoch=271/300,42/43of train, loss=473101.46875\n",
      "epoch=272/300,0/43of train, loss=505047.8125\n",
      "epoch=272/300,21/43of train, loss=437373.0625\n",
      "epoch=272/300,42/43of train, loss=488416.9375\n",
      "epoch=273/300,0/43of train, loss=586020.625\n",
      "epoch=273/300,21/43of train, loss=456328.03125\n",
      "epoch=273/300,42/43of train, loss=479076.6875\n",
      "epoch=274/300,0/43of train, loss=494000.28125\n",
      "epoch=274/300,21/43of train, loss=486569.34375\n",
      "epoch=274/300,42/43of train, loss=488086.96875\n",
      "epoch=275/300,0/43of train, loss=486139.125\n",
      "epoch=275/300,21/43of train, loss=479828.46875\n",
      "epoch=275/300,42/43of train, loss=507654.25\n",
      "epoch=276/300,0/43of train, loss=502942.71875\n",
      "epoch=276/300,21/43of train, loss=478306.21875\n",
      "epoch=276/300,42/43of train, loss=513371.9375\n",
      "epoch=277/300,0/43of train, loss=528262.5625\n",
      "epoch=277/300,21/43of train, loss=477894.53125\n",
      "epoch=277/300,42/43of train, loss=518326.71875\n",
      "epoch=278/300,0/43of train, loss=461382.78125\n",
      "epoch=278/300,21/43of train, loss=436447.875\n",
      "epoch=278/300,42/43of train, loss=503912.84375\n",
      "epoch=279/300,0/43of train, loss=481688.5\n",
      "epoch=279/300,21/43of train, loss=524229.0625\n",
      "epoch=279/300,42/43of train, loss=515320.6875\n",
      "epoch=280/300,0/43of train, loss=485435.0\n",
      "epoch=280/300,21/43of train, loss=456595.4375\n",
      "epoch=280/300,42/43of train, loss=597661.0\n"
     ]
    }
   ],
   "source": [
    "# Training and Saving\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "  model.train()\n",
    "  train_epoch_loss = []\n",
    "  for idx,(data_x,data_y) in enumerate(train_loader,0):\n",
    "      data_x = data_x.to(torch.float32).to(args.device)\n",
    "      data_y = data_y.to(torch.float32).to(args.device)\n",
    "      outputs = model(data_x)\n",
    "      optimizer.zero_grad()\n",
    "      loss = criterion(data_y, outputs)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_epoch_loss.append(loss.item())\n",
    "      train_loss.append(loss.item())\n",
    "      if idx%(len(train_loader)//2)==0:\n",
    "          print(\"epoch={}/{},{}/{}of train, loss={}\".format(\n",
    "              epoch, args.epochs, idx, len(train_loader),loss.item()))\n",
    "  train_epochs_loss.append(np.average(train_epoch_loss))\n",
    "  \n",
    "  #=====================valid============================\n",
    "  model.eval()\n",
    "  valid_epoch_loss = []\n",
    "  for idx,(data_x,data_y) in enumerate(valid_loader,0):\n",
    "      data_x = data_x.to(torch.float32).to(args.device)\n",
    "      data_y = data_y.to(torch.float32).to(args.device)\n",
    "      outputs = model(data_x)\n",
    "      loss = criterion(outputs,data_y)\n",
    "      valid_epoch_loss.append(loss.item())\n",
    "      valid_loss.append(loss.item())\n",
    "  valid_epochs_loss.append(np.average(valid_epoch_loss))\n",
    "  #==================early stopping======================\n",
    "  # early_stopping(valid_epochs_loss[-1],model=MyModel,path=r'')\n",
    "  # if early_stopping.early_stop:\n",
    "  #    print(\"Early stopping\")\n",
    "  #    break\n",
    "  #====================adjust lr========================\n",
    "  lr_adjust = {\n",
    "          2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "          10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "      }\n",
    "  #if epoch in lr_adjust.keys():\n",
    "  #    lr = lr_adjust[epoch]\n",
    "  #    for param_group in optimizer.param_groups:\n",
    "  #        param_group['lr'] = lr\n",
    "  #    print('Updating learning rate to {}'.format(lr))\n",
    "\n",
    "torch.save(mymodel, '../Models/v0_ep300_lr001.pth')\n",
    "end = time.time()\n",
    "print(\"Total training time:\", end-start, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1q7NOncq_U7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOu2+56/iaay+3ugFmvPoFz",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
