{"cells":[{"cell_type":"markdown","metadata":{"id":"ozVSXkfVnttc"},"source":["### Model1 CNN+LSTM"]},{"cell_type":"markdown","metadata":{"id":"7vcLcTthG3-3"},"source":["## Implementation detals\n","- Input size:\n","- Output size:\n","<br></br>\n","- Training data:\n","- Validation data:\n","- Test data:\n","<br></br>\n","- Number of epoch:\n","- Batch size:\n","- Learning rate:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7645,"status":"ok","timestamp":1671664006567,"user":{"displayName":"曹冠宇","userId":"04420153168815849534"},"user_tz":-540},"id":"Ruor2R1Gn000","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ef6b4f9c-e0c9-492d-cf54-51c7b279b709"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchinfo\n","  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.7.1\n"]}],"source":["# Imports\n","!pip install torchinfo\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as Data\n","import torchvision\n","from torchinfo import summary\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n"]},{"cell_type":"markdown","metadata":{"id":"rnZopCypFbgq"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNGMVyA-KvTt"},"outputs":[],"source":["# Model\n","# input_size = (30 * 3) * 33\n","# mid_size = 128 * 1\n","# output_size = 200 * 120\n","\n","class MyEncodeCNN(nn.Module):\n","  def __init__(self):\n","    super(MyEncodeCNN, self).__init__()\n","\n","    self.layer1 = nn.Sequential(\n","        nn.Conv2d(1, 32, kernel_size=3, stride=(3, 1), padding=0),\n","        nn.BatchNorm2d(32),\n","        nn.LeakyReLU(inplace=True),\n","        # No Padding\n","        # No Pooling\n","        # In = 90 * 33 * 1\n","        # Out = 30 * 31 * 32\n","    )\n","\n","    self.layer2 = nn.Sequential(\n","        nn.Conv2d(32, 64, kernel_size=5, stride=(1, 1), padding=0),\n","        nn.BatchNorm2d(64),\n","        nn.LeakyReLU(inplace=True),\n","        # No Padding\n","        # No Pooling\n","        # In = 30 * 31 * 32\n","        # Out = 26 * 27 * 64\n","    )\n","\n","    self.layer3 = nn.Sequential(\n","        nn.Conv2d(64, 32, kernel_size=5, stride=(1, 1), padding=0),\n","        nn.BatchNorm2d(32),\n","        nn.LeakyReLU(inplace=True),\n","        # No Padding\n","        # No Pooling\n","        # In = 26 * 27 * 64\n","        # Out = 22 * 23 * 32\n","    )\n","\n","    self.layer4 = nn.Sequential(\n","        nn.Conv2d(32, 1, kernel_size=5, stride=(1, 1), padding=0),\n","        nn.BatchNorm2d(1),\n","        nn.LeakyReLU(inplace=True),\n","        # No Padding\n","        # No Pooling\n","        # In = 22 * 23 * 32\n","        # Out = 18 * 19 * 1\n","    )\n","\n","    self.layerfc = nn.Sequential(\n","        nn.Linear(18*19, 128),\n","        nn.ReLU(inplace=True),\n","        nn.Linear(128, 32)\n","    )\n","\n","    self.dropout = nn.Dropout(p=0.5)\n","\n","  def forward(self, x):\n","    x = self.layer1(x)\n","    x = self.layer2(x)\n","    x = self.layer3(x)\n","    x = self.layer4(x)\n","\n","    # size_x = batch_size * 18 * 19\n","    # x = self.dropout(x)\n","    # x = self.layerfc(x)\n","    return x\n","\n","\n","class MyEncodeLSTM(nn.Module):\n","  def __init__(self):\n","    super(MyEncodeLSTM, self).__init__()\n","\n","    self.hidden_size = 128\n","    self.num_layers = 2\n","\n","    self.layer = nn.LSTM(36, self.hidden_size, self.num_layers, batch_first=True, dropout=0.1)\n","\n","  def forward(self, x):\n","    out, (final_hidden_state, final_cell_state)  = self.layer(x)\n","    return out[:, None, -1, :]  \n","\n","\n","class MyEncoder(nn.Module):\n","  def __init__(self):\n","    super(MyEncoder, self).__init__()\n","  \n","    self.cnn1 = MyEncodeCNN()\n","    self.cnn2 = MyEncodeCNN()\n","    self.lstm = MyEncodeLSTM()\n","\n","  def forward(self, x):\n","    x = torch.chunk(x, 2, dim=0)\n","    x1 = self.cnn1.forward(x[0].view(-1, 1, 90, 33))\n","    x2 = self.cnn2.forward(x[1].view(-1, 1, 90, 33))\n","\n","    # size_x = batch_size * 18 * 19\n","    \n","    x = torch.cat([x1, x2], dim=2).transpose(1, 2).view(-1, 19, 36)\n","    # size_x = batch_size * 36  * 19\n","\n","    x = self.lstm.forward(x)\n","    return x\n","\n","\n","class MyDecodeLSTM(nn.Module):\n","  def __init__(self):\n","    super(MyDecodeLSTM, self).__init__()\n","\n","    self.hidden_size = 64\n","    self.num_layers = 2\n","    self.hidden = self.init_hidden()\n","\n","    self.layer = nn.LSTM(128, self.hidden_size, self.num_layers, batch_first=True, dropout=0.1)\n","\n","  def init_hidden(self):\n","    # Only for initialization\n","    return (torch.zeros(1, 1, self.hidden_size), torch.zeros(1, 1, self.hidden_size))\n","\n","  def forward(self, x):\n","    out, (final_hidden_state, final_cell_state)  = self.layer(x)\n","    return out[:, -1, :]\n","\n","\n","class MyDecodeCNN(nn.Module):\n","  def __init__(self):\n","    super(MyDecodeCNN, self).__init__()\n","\n","    self.layerfc = nn.Sequential(\n","        nn.Linear(64, 256),\n","        nn.ReLU(inplace=True),\n","        nn.Linear(256, 1344)\n","    )\n","\n","    self.layer1 = nn.Sequential(\n","        nn.ConvTranspose2d(1, 32, kernel_size=4, stride=2, padding=0),\n","        nn.BatchNorm2d(32),\n","        nn.LeakyReLU(inplace=True),\n","        # No Unpooling\n","        # Upsample size by 2x2\n","        # In = 15 * 25 * 1\n","        # Out = 30 * 50 * 32\n","    )\n","\n","    self.layer2 = nn.Sequential(\n","        nn.ConvTranspose2d(32, 64, kernel_size=4, stride=2, padding=1),\n","        nn.BatchNorm2d(64),\n","        nn.LeakyReLU(inplace=True),\n","        # No Unpooling\n","        # Upsample size by 2x2\n","        # In = 30 * 50 * 32\n","        # Out = 60 * 100 * 64\n","    )\n","\n","    self.layer3 = nn.Sequential(\n","        nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=0),\n","        nn.BatchNorm2d(32),\n","        nn.LeakyReLU(inplace=True),\n","        # No Unpooling\n","        # Upsample size by 2x2\n","        # In = 60 * 100 * 64\n","        # Out = 120 * 200 * 32\n","    )\n","\n","    self.layer4 = nn.Sequential(\n","        nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=0),\n","        nn.BatchNorm2d(1),\n","        nn.LeakyReLU(inplace=True),\n","        # No Unpooling\n","        # Upsample size by 2x2\n","        # In = 120 * 200 * 32\n","        # Out = 240 * 400 * 1\n","    )\n","\n","    self.dropout = nn.Dropout(p=0.5)\n","\n","  def forward(self, x):\n","    x = self.layerfc(x)\n","    x = self.dropout(x)\n","    x = self.layer1(x.view(-1, 1, 28, 48))\n","    x = self.layer2(x)\n","    x = self.layer3(x)\n","    x = self.layer4(x)\n","\n","    # size_x = batch_size * 200 * 120\n","\n","    return x\n","\n","\n","class MyDecoder(nn.Module):\n","  def __init__(self):\n","    super(MyDecoder, self).__init__()\n","  \n","    self.lstm = MyDecodeLSTM()\n","    self.cnn = MyDecodeCNN()\n","\n","  def forward(self, x):\n","    x = self.lstm.forward(x)\n","\n","    # size_x = batch_size * 375, need batch_size * 25 * 15\n","\n","    x = self.cnn.forward(x)\n","    return x\n","\n","\n","class MyModel(nn.Module):\n","  def __init__(self):\n","    super(MyModel, self).__init__()\n","  \n","    self.encoder = MyEncoder()\n","    self.decoder = MyDecoder()\n","\n","  def forward(self, x):\n","    z = self.encoder(x)\n","    y = self.decoder(z) \n","    return y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":270,"status":"ok","timestamp":1671636390748,"user":{"displayName":"曹冠宇","userId":"04420153168815849534"},"user_tz":-540},"id":"C7tCrrQqvV3i","outputId":"55705461-a3cc-4596-de22-348c04d96e2d"},"outputs":[{"data":{"text/plain":["===============================================================================================\n","Layer (type:depth-idx)                        Output Shape              Param #\n","===============================================================================================\n","MyModel                                       [1, 1, 120, 200]          --\n","├─MyEncoder: 1-1                              [1, 1, 128]               --\n","│    └─MyEncodeCNN: 2-1                       --                        48,032\n","│    │    └─Sequential: 3-1                   [1, 32, 30, 31]           384\n","│    │    └─Sequential: 3-2                   [1, 64, 26, 27]           51,392\n","│    │    └─Sequential: 3-3                   [1, 32, 22, 23]           51,296\n","│    │    └─Sequential: 3-4                   [1, 1, 18, 19]            803\n","│    └─MyEncodeCNN: 2-2                       --                        48,032\n","│    │    └─Sequential: 3-5                   [1, 32, 30, 31]           384\n","│    │    └─Sequential: 3-6                   [1, 64, 26, 27]           51,392\n","│    │    └─Sequential: 3-7                   [1, 32, 22, 23]           51,296\n","│    │    └─Sequential: 3-8                   [1, 1, 18, 19]            803\n","│    └─MyEncodeLSTM: 2-3                      --                        --\n","│    │    └─LSTM: 3-9                         [1, 19, 128]              217,088\n","├─MyDecoder: 1-2                              [1, 1, 120, 200]          --\n","│    └─MyDecodeLSTM: 2-4                      --                        --\n","│    │    └─LSTM: 3-10                        [1, 1, 64]                82,944\n","│    └─MyDecodeCNN: 2-5                       --                        --\n","│    │    └─Sequential: 3-11                  [1, 1344]                 362,048\n","│    │    └─Dropout: 3-12                     [1, 1344]                 --\n","│    │    └─Sequential: 3-13                  [1, 32, 58, 98]           608\n","│    │    └─Sequential: 3-14                  [1, 64, 116, 196]         32,960\n","│    │    └─Sequential: 3-15                  [1, 32, 118, 198]         18,528\n","│    │    └─Sequential: 3-16                  [1, 1, 120, 200]          291\n","===============================================================================================\n","Total params: 1,018,281\n","Trainable params: 1,018,281\n","Non-trainable params: 0\n","Total mult-adds (G): 1.32\n","===============================================================================================\n","Input size (MB): 0.02\n","Forward/backward pass size (MB): 41.49\n","Params size (MB): 3.69\n","Estimated Total Size (MB): 45.20\n","==============================================================================================="]},"execution_count":468,"metadata":{},"output_type":"execute_result"}],"source":["m1 = MyModel()\n","summary(m1, input_size=(2, 90, 33))"]},{"cell_type":"markdown","metadata":{"id":"ZzNX5ISCFhBY"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18918,"status":"ok","timestamp":1671664037687,"user":{"displayName":"曹冠宇","userId":"04420153168815849534"},"user_tz":-540},"id":"3lkvH24vGZ8t","outputId":"21098cf0-9111-4d5d-99f2-b9d453f6fe84"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount Drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10574,"status":"ok","timestamp":1671664054419,"user":{"displayName":"曹冠宇","userId":"04420153168815849534"},"user_tz":-540},"id":"Qv4H17YcAR3I","outputId":"43bbd323-54c0-47a3-e21e-3d2029ba7bf3"},"outputs":[{"output_type":"stream","name":"stdout","text":["loaded\n","2730 780 390\n","43 13 7\n"]}],"source":["# My Dataset\n","\n","class MyDataset(Data.Dataset):\n","    def __init__(self, x_path, y_path):\n","        self.data = self.load_data(x_path, y_path)\n","        print('loaded')\n","\n","    def __getitem__(self, index):\n","        return self.data['x'][index], self.data['y'][index]\n","\n","    def __len__(self):\n","        return self.data['x'].shape[0]\n","\n","    def load_data(self, x_path, y_path):\n","        x = np.load(x_path)\n","        y = np.load(y_path)\n","\n","        if x.shape[0] == y.shape[0]:\n","            total_count = x.shape[0]\n","        else:\n","            print(x.shape, y.shape, \"lengths not equal!\")\n","\n","        return {'x':x, 'y':y}\n","    \n","\n","mydata = MyDataset('dataset/depth_3m/x.npy', 'dataset/depth_3m/y.npy')\n","\n","train_size = int(len(mydata) * 0.7)\n","valid_size = int(len(mydata) * 0.2)\n","test_size = int(len(mydata)) - train_size - valid_size\n","train_dataset, valid_dataset, test_dataset = Data.random_split(mydata, [train_size, valid_size, test_size])\n","print(train_size, valid_size, test_size)\n","\n","train_loader = Data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n","valid_loader = Data.DataLoader(valid_dataset, batch_size=64, shuffle=True)\n","test_loader = Data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n","\n","print(len(train_loader), len(valid_loader), len(test_loader))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6H9dFQc8LhN1"},"outputs":[],"source":["print(mydata.data['x'].shape)\n","print(mydata.data['y'].shape)"]},{"cell_type":"markdown","metadata":{"id":"xy-67GwAFj5T"},"source":["## Running gears"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5GAqZF5D3lL"},"outputs":[],"source":["# Hyperparameters\n","\n","class MyArgs:\n","  def __init__(self, epochs=10, learning_rate=0.001):\n","    self.epochs = epochs\n","    self.learning_rate = learning_rate\n","    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","args = MyArgs(epochs=40)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lCvRd4pOFBzc"},"outputs":[],"source":["# Model and Loss\n","\n","model = MyModel().to(args.device)\n","\n","criterion = torch.nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n","\n","train_loss = []\n","valid_loss = []\n","train_epochs_loss = []\n","valid_epochs_loss = []\n","\n","# early_stopping = EarlyStopping(patience=args.patience,verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UljtLCZzFR4P","outputId":"02d3671f-7293-422f-de88-c0bd239043bf"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64, 1, 120, 200])) that is different to the input size (torch.Size([64, 120, 200])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch=0/40,0/43of train, loss=173724464.0\n","epoch=0/40,21/43of train, loss=175918160.0\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([42, 1, 120, 200])) that is different to the input size (torch.Size([42, 120, 200])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch=0/40,42/43of train, loss=178626656.0\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64, 120, 200])) that is different to the input size (torch.Size([64, 1, 120, 200])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([12, 120, 200])) that is different to the input size (torch.Size([12, 1, 120, 200])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch=1/40,0/43of train, loss=175591616.0\n","epoch=1/40,21/43of train, loss=173874608.0\n","epoch=1/40,42/43of train, loss=178819248.0\n","epoch=2/40,0/43of train, loss=178741136.0\n","epoch=2/40,21/43of train, loss=175546608.0\n","epoch=2/40,42/43of train, loss=181259312.0\n","Updating learning rate to 5e-05\n","epoch=3/40,0/43of train, loss=178398160.0\n","epoch=3/40,21/43of train, loss=173828592.0\n","epoch=3/40,42/43of train, loss=169212864.0\n","epoch=4/40,0/43of train, loss=179550304.0\n","epoch=4/40,21/43of train, loss=172972240.0\n","epoch=4/40,42/43of train, loss=175734944.0\n","Updating learning rate to 1e-05\n","epoch=5/40,0/43of train, loss=170021248.0\n","epoch=5/40,21/43of train, loss=171634096.0\n","epoch=5/40,42/43of train, loss=181930304.0\n","epoch=6/40,0/43of train, loss=174274736.0\n","epoch=6/40,21/43of train, loss=177886416.0\n","epoch=6/40,42/43of train, loss=178265488.0\n","Updating learning rate to 5e-06\n","epoch=7/40,0/43of train, loss=173496176.0\n","epoch=7/40,21/43of train, loss=173378416.0\n","epoch=7/40,42/43of train, loss=174926576.0\n","epoch=8/40,0/43of train, loss=177320016.0\n","epoch=8/40,21/43of train, loss=170998544.0\n","epoch=8/40,42/43of train, loss=178157744.0\n","Updating learning rate to 1e-06\n","epoch=9/40,0/43of train, loss=177182768.0\n","epoch=9/40,21/43of train, loss=179149408.0\n","epoch=9/40,42/43of train, loss=169195664.0\n","epoch=10/40,0/43of train, loss=174771952.0\n","epoch=10/40,21/43of train, loss=182313728.0\n","epoch=10/40,42/43of train, loss=178862736.0\n","Updating learning rate to 5e-07\n","epoch=11/40,0/43of train, loss=170558960.0\n","epoch=11/40,21/43of train, loss=176259088.0\n","epoch=11/40,42/43of train, loss=186967856.0\n","epoch=12/40,0/43of train, loss=167063872.0\n","epoch=12/40,21/43of train, loss=175103776.0\n","epoch=12/40,42/43of train, loss=184496624.0\n","epoch=13/40,0/43of train, loss=180650592.0\n","epoch=13/40,21/43of train, loss=180230928.0\n","epoch=13/40,42/43of train, loss=178595792.0\n","epoch=14/40,0/43of train, loss=177638976.0\n","epoch=14/40,21/43of train, loss=174423792.0\n","epoch=14/40,42/43of train, loss=174213232.0\n","epoch=15/40,0/43of train, loss=171817120.0\n","epoch=15/40,21/43of train, loss=175898816.0\n","epoch=15/40,42/43of train, loss=185864352.0\n","Updating learning rate to 1e-07\n","epoch=16/40,0/43of train, loss=174115808.0\n","epoch=16/40,21/43of train, loss=180648656.0\n","epoch=16/40,42/43of train, loss=170334240.0\n","epoch=17/40,0/43of train, loss=174231024.0\n","epoch=17/40,21/43of train, loss=181401936.0\n","epoch=17/40,42/43of train, loss=179142896.0\n","epoch=18/40,0/43of train, loss=172954864.0\n","epoch=18/40,21/43of train, loss=177347008.0\n","epoch=18/40,42/43of train, loss=175500096.0\n","epoch=19/40,0/43of train, loss=177903760.0\n","epoch=19/40,21/43of train, loss=176682784.0\n","epoch=19/40,42/43of train, loss=178186752.0\n","epoch=20/40,0/43of train, loss=181297024.0\n","epoch=20/40,21/43of train, loss=168979840.0\n","epoch=20/40,42/43of train, loss=168941936.0\n","Updating learning rate to 5e-08\n","epoch=21/40,0/43of train, loss=177569808.0\n","epoch=21/40,21/43of train, loss=175858960.0\n","epoch=21/40,42/43of train, loss=171005920.0\n","epoch=22/40,0/43of train, loss=173816160.0\n","epoch=22/40,21/43of train, loss=177150304.0\n","epoch=22/40,42/43of train, loss=176357840.0\n","epoch=23/40,0/43of train, loss=181812720.0\n","epoch=23/40,21/43of train, loss=175954112.0\n"]}],"source":["# Training and Saving\n","\n","for epoch in range(args.epochs):\n","  model.train()\n","  train_epoch_loss = []\n","  for idx,(data_x,data_y) in enumerate(train_loader,0):\n","      data_x = data_x.to(torch.float32).to(args.device)\n","      data_y = data_y.to(torch.float32).to(args.device)\n","      outputs = model(data_x)\n","      optimizer.zero_grad()\n","      loss = criterion(data_y, outputs)\n","      loss.backward()\n","      optimizer.step()\n","      train_epoch_loss.append(loss.item())\n","      train_loss.append(loss.item())\n","      if idx%(len(train_loader)//2)==0:\n","          print(\"epoch={}/{},{}/{}of train, loss={}\".format(\n","              epoch, args.epochs, idx, len(train_loader),loss.item()))\n","  train_epochs_loss.append(np.average(train_epoch_loss))\n","  \n","  #=====================valid============================\n","  model.eval()\n","  valid_epoch_loss = []\n","  for idx,(data_x,data_y) in enumerate(valid_loader,0):\n","      data_x = data_x.to(torch.float32).to(args.device)\n","      data_y = data_y.to(torch.float32).to(args.device)\n","      outputs = model(data_x)\n","      loss = criterion(outputs,data_y)\n","      valid_epoch_loss.append(loss.item())\n","      valid_loss.append(loss.item())\n","  valid_epochs_loss.append(np.average(valid_epoch_loss))\n","  #==================early stopping======================\n","  # early_stopping(valid_epochs_loss[-1],model=MyModel,path=r'')\n","  # if early_stopping.early_stop:\n","  #    print(\"Early stopping\")\n","  #    break\n","  #====================adjust lr========================\n","  lr_adjust = {\n","          2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n","          10: 5e-7, 15: 1e-7, 20: 5e-8\n","      }\n","  if epoch in lr_adjust.keys():\n","      lr = lr_adjust[epoch]\n","      for param_group in optimizer.param_groups:\n","          param_group['lr'] = lr\n","      print('Updating learning rate to {}'.format(lr))\n","\n","torch.save(MyModel, '/Models/ep40.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1q7NOncq_U7"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOu2+56/iaay+3ugFmvPoFz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}